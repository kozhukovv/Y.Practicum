{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект по определению токсичности комментариев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель работы заключается в обучении модели классификации комментариев на позитивные и негативные. Требуется построить модель со значением метрики качества *F1* не меньше 0.75.\n",
    "\n",
    "**Описание данных**\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим все необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T08:19:53.121745Z",
     "start_time": "2023-06-16T08:19:51.313418Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install catboost --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T08:19:55.250156Z",
     "start_time": "2023-06-16T08:19:53.126149Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aleksandrkozuhov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aleksandrkozuhov/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/aleksandrkozuhov/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aleksandrkozuhov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/aleksandrkozuhov/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score as f1\n",
    "\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выгрузим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T08:19:55.774242Z",
     "start_time": "2023-06-16T08:19:55.252407Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('toxic_comments.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим имеющиеся данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T08:19:55.780959Z",
     "start_time": "2023-06-16T08:19:55.775548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                     text  toxic\n0       Explanation\\nWhy the edits made under my usern...      0\n1       D'aww! He matches this background colour I'm s...      0\n2       Hey man, I'm really not trying to edit war. It...      0\n3       \"\\nMore\\nI can't make any real suggestions on ...      0\n4       You, sir, are my hero. Any chance you remember...      0\n...                                                   ...    ...\n159446  \":::::And for the second time of asking, when ...      0\n159447  You should be ashamed of yourself \\n\\nThat is ...      0\n159448  Spitzer \\n\\nUmm, theres no actual article for ...      0\n159449  And it looks like it was actually you who put ...      0\n159450  \"\\nAnd ... I really don't think you understand...      0\n\n[159292 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>159446</th>\n      <td>\":::::And for the second time of asking, when ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159447</th>\n      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159448</th>\n      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159449</th>\n      <td>And it looks like it was actually you who put ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159450</th>\n      <td>\"\\nAnd ... I really don't think you understand...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>159292 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T08:19:55.806312Z",
     "start_time": "2023-06-16T08:19:55.781885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим данные на дубликаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-06-16T08:19:56.004428Z",
     "start_time": "2023-06-16T08:19:55.840184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, дубликатов нет. Лемматизируем текст с помощью WordNetLemmatizer из библиотеки NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T08:19:56.007563Z",
     "start_time": "2023-06-16T08:19:56.006145Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', sentence)\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemmatized_output = \" \".join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_list])\n",
    "    return \" \".join(lemmatized_output.split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T08:28:42.739769Z",
     "start_time": "2023-06-16T08:19:56.010494Z"
    }
   },
   "outputs": [],
   "source": [
    "df['lemm_text'] = df['text'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-06-16T08:28:42.771578Z",
     "start_time": "2023-06-16T08:28:42.744845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                     text  toxic  \\\n47767         you will be blocked from editing Wikipedia.      0   \n11183   \"\\n\\n Guy de Lusignan \\n\\nCome on Adam, I was ...      0   \n54598   \"\\n\\n HI! \\n\\nYess its me Pralph, I was gonna ...      0   \n149886  \"\\nThe addition was a \"\"wikipedia situation wh...      0   \n32833   ), and created a Philosophy lecturer sub-secti...      0   \n\n                                                lemm_text  \n47767               you will be block from edit wikipedia  \n11183   guy de lusignan come on adam i be surprised by...  \n54598   hi yes it me pralph i be gon na authros some s...  \n149886  the addition be a wikipedia situation where a ...  \n32833   and create a philosophy lecturer sub section u...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>toxic</th>\n      <th>lemm_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>47767</th>\n      <td>you will be blocked from editing Wikipedia.</td>\n      <td>0</td>\n      <td>you will be block from edit wikipedia</td>\n    </tr>\n    <tr>\n      <th>11183</th>\n      <td>\"\\n\\n Guy de Lusignan \\n\\nCome on Adam, I was ...</td>\n      <td>0</td>\n      <td>guy de lusignan come on adam i be surprised by...</td>\n    </tr>\n    <tr>\n      <th>54598</th>\n      <td>\"\\n\\n HI! \\n\\nYess its me Pralph, I was gonna ...</td>\n      <td>0</td>\n      <td>hi yes it me pralph i be gon na authros some s...</td>\n    </tr>\n    <tr>\n      <th>149886</th>\n      <td>\"\\nThe addition was a \"\"wikipedia situation wh...</td>\n      <td>0</td>\n      <td>the addition be a wikipedia situation where a ...</td>\n    </tr>\n    <tr>\n      <th>32833</th>\n      <td>), and created a Philosophy lecturer sub-secti...</td>\n      <td>0</td>\n      <td>and create a philosophy lecturer sub section u...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее следует векторизировать данные для работы. Для этого рассчитаем величины TF-IDF, но перед этим разделим данные на тренировочную и тестовую выборки для кросс-валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "df_tr, df_new = train_test_split(df, test_size=.2, random_state=12345, stratify=df['toxic'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T08:28:42.824266Z",
     "start_time": "2023-06-16T08:28:42.765282Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_new, test_size=.2, random_state=12345)\n",
    "\n",
    "target_train = train['toxic']\n",
    "target_test = test['toxic']\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords.words('english'), ngram_range=(1,2))\n",
    "\n",
    "train_tf_idf = count_tf_idf.fit_transform(train['lemm_text'])\n",
    "test_tf_idf = count_tf_idf.transform(test['lemm_text'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T08:28:44.682257Z",
     "start_time": "2023-06-16T08:28:42.803735Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим 2 модели: логистическую регрессию и градиентный бустинг LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T08:39:31.986587Z",
     "start_time": "2023-06-16T08:37:45.037384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 7 candidates, totalling 70 fits\n",
      "Логистическая регрессия\n",
      "Скорость обучения составила 106.93 секунд\n",
      "Лучшие параметры: {'C': 4}\n",
      "f1-мера по валидационной выборке: 0.72\n"
     ]
    }
   ],
   "source": [
    "params_reg = {\n",
    "    'C': [.1,.5,1,2,3,4,5]\n",
    "}\n",
    "\n",
    "model_log_reg = LogisticRegression(random_state=12345, max_iter=150, class_weight='balanced')\n",
    "\n",
    "model_l_r = GridSearchCV(estimator=model_log_reg,\n",
    "                         param_grid=params_reg,\n",
    "                         cv=10,\n",
    "                         n_jobs=-1,\n",
    "                         scoring='f1',\n",
    "                         verbose=3\n",
    "                         )\n",
    "\n",
    "s = time.time()\n",
    "model_l_r.fit(train_tf_idf, target_train)\n",
    "e = time.time()\n",
    "print(f'Логистическая регрессия')\n",
    "print(f'Скорость обучения составила {round(e-s,2)} секунд')\n",
    "print(f'Лучшие параметры: {model_l_r.best_params_}')\n",
    "print(f'f1-мера по валидационной выборке: {round(model_l_r.best_score_,2)}')\n",
    "#print(f'f1-мера по тестовой выборке: {round(f1(target_test, predictions_test),2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T08:45:25.156555Z",
     "start_time": "2023-06-16T08:40:46.903796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "LGBM классификация\n",
      "Скорость обучения: 278.24\n",
      "Лучшие параметры: {'learning_rate': 0.1, 'n_estimators': 500}\n",
      "f1 для лучших параметров на валидационной выборке: 0.7\n"
     ]
    }
   ],
   "source": [
    "params_lgb = {\n",
    "    'learning_rate': [.01,.03,.1,.15],\n",
    "    'n_estimators': [100,500,1000],\n",
    "}\n",
    "\n",
    "model = lgb.LGBMClassifier(class_weight='balanced', random_state=12345,)\n",
    "estimator=model,\n",
    "model_lgb = GridSearchCV(estimator=model,\n",
    "                         param_grid=params_lgb,\n",
    "                         cv=5,\n",
    "                         n_jobs=-1,\n",
    "                         scoring='f1',\n",
    "                         verbose=3\n",
    "                         )\n",
    "\n",
    "s = time.time()\n",
    "model_lgb.fit(train_tf_idf, target_train)\n",
    "e = time.time()\n",
    "print('LGBM классификация')\n",
    "print(f'Скорость обучения: {round(e-s,2)}')\n",
    "print(f'Лучшие параметры: {model_lgb.best_params_}')\n",
    "print(f'f1 для лучших параметров на валидационной выборке:', round(model_lgb.best_score_,2))\n",
    "#print(f'f1 для лучших параметров на тестовой выборке:', round(f1(target_test, predictions_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Лучше всего показала себя модель логистической регрессии, обучим ее на тестовой выборке"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Скорость обучения: 2.5\n",
      "Скорость предсказания модели на тестовой выборке: 0.0\n",
      "f1 для лучших параметров на тестовой выборке: 0.77\n"
     ]
    }
   ],
   "source": [
    "model_log_reg = LogisticRegression(random_state=12345, max_iter=150, class_weight='balanced', C=4)\n",
    "\n",
    "s = time.time()\n",
    "model_log_reg.fit(train_tf_idf, target_train)\n",
    "e = time.time()\n",
    "print(f'Скорость обучения: {round(e-s,2)}')\n",
    "\n",
    "s = time.time()\n",
    "predictions_test = model_log_reg.predict(test_tf_idf)\n",
    "e = time.time()\n",
    "print(f'Скорость предсказания модели на тестовой выборке: {round(e-s,2)}')\n",
    "print(f'f1 для лучших параметров на тестовой выборке:', round(f1(target_test, predictions_test),2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T08:53:37.164712Z",
     "start_time": "2023-06-16T08:53:34.664849Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Были подготовлены данные по текстам комментариев для обучения, проанализированы два метода машинного обучения.\n",
    "\n",
    "Установлено, что лучше себя показала модель логистической регрессии, f1 на тестовой выборке составила 0.77"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 2799,
    "start_time": "2023-06-15T06:52:01.862Z"
   },
   {
    "duration": 8886,
    "start_time": "2023-06-15T06:52:04.665Z"
   },
   {
    "duration": 2244,
    "start_time": "2023-06-15T06:52:13.552Z"
   },
   {
    "duration": 13,
    "start_time": "2023-06-15T06:52:15.798Z"
   },
   {
    "duration": 35,
    "start_time": "2023-06-15T06:52:15.813Z"
   },
   {
    "duration": 2541,
    "start_time": "2023-06-15T06:52:39.604Z"
   },
   {
    "duration": 3085,
    "start_time": "2023-06-15T06:52:42.148Z"
   },
   {
    "duration": 2475,
    "start_time": "2023-06-15T06:52:45.236Z"
   },
   {
    "duration": 14,
    "start_time": "2023-06-15T06:52:47.714Z"
   },
   {
    "duration": 54,
    "start_time": "2023-06-15T06:52:47.730Z"
   },
   {
    "duration": 39,
    "start_time": "2023-06-15T06:52:47.793Z"
   },
   {
    "duration": 268,
    "start_time": "2023-06-15T06:52:47.834Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-15T06:52:48.104Z"
   },
   {
    "duration": 1357,
    "start_time": "2023-06-15T06:52:48.111Z"
   },
   {
    "duration": 11,
    "start_time": "2023-06-15T06:52:49.475Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
